â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘          ğŸ¯ AUDIO-TEXT SYNCHRONIZATION EXPLAINED ğŸ¯          â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUESTION: How are audio and text captions synchronized?

ANSWER: We use OpenAI Whisper STT (Speech-to-Text)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š THE COMPLETE FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: GENERATE AUDIO (ElevenLabs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each dialogue line:
  â†’ Peter: "Y'know, Stewie..." â†’ segment_0.mp3 (3.2s)
  â†’ Stewie: "Indeed, Peter..." â†’ segment_1.mp3 (4.1s)
  â†’ Peter: "Heh, yeah..." â†’ segment_2.mp3 (2.5s)
  â†’ Stewie: "A noble..." â†’ segment_3.mp3 (3.8s)

Step 2: CONCATENATE (with 1s gaps)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
segment_0 (0.0 - 3.2s)
  [1 second gap]
segment_1 (4.2 - 8.3s)
  [1 second gap]
segment_2 (9.3 - 11.8s)
  [1 second gap]
segment_3 (12.8 - 16.6s)

Result: final_audio.mp3 (16.6 seconds)

Step 3: GENERATE CAPTIONS (Whisper)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Send final_audio.mp3 to OpenAI Whisper

Whisper returns:
  - Full transcription
  - Word-level timestamps

Example output:
  "Y'know" â†’ 0.00 - 0.52s
  "Stewie" â†’ 0.52 - 1.08s
  "OpenAI" â†’ 1.08 - 1.64s
  "is" â†’ 1.64 - 1.82s
  [1s silence gap here - Whisper handles it]
  "Indeed" â†’ 4.20 - 4.78s
  "Peter" â†’ 4.78 - 5.24s
  ...

Step 4: VIDEO COMPOSITION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Load background video
2. Loop if too short (auto-loop feature)
3. Crop to 9:16 vertical
4. Sync final_audio.mp3
5. Overlay captions using Whisper timestamps
6. Render final MP4

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… WHY WHISPER?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ SIMPLE: Works automatically, no manual timing
âœ“ ACCURATE: State-of-the-art speech recognition
âœ“ HANDLES EVERYTHING: Gaps, concatenation, all automatic
âœ“ PROVEN: Already working in your Topic Mode
âœ“ COST-EFFECTIVE: ~$0.006 per minute

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”„ ALTERNATIVE: ElevenLabs Timestamps
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ElevenLabs DOES offer timestamps via convert_with_timestamps API

Advantages:
  âœ“ Character-level precision
  âœ“ Direct from TTS source

Challenges:
  âœ— Need to manually track cumulative timing
  âœ— Must account for 1s gaps we added
  âœ— Convert character â†’ word level
  âœ— More complex implementation

Example calculation needed:
  Segment 0: 0.0 - 3.2s
  + 1s gap
  Segment 1: Need to offset by 4.2s (3.2 + 1.0)
  + 1s gap
  Segment 2: Need to offset by 9.3s (3.2 + 1.0 + 4.1 + 1.0)
  ...and so on

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ CURRENT IMPLEMENTATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

File: app.py â†’ generate_video_script_mode()

# Generate audio with ElevenLabs
audio_result = generate_dialogue_audio(
    elevenlabs_api_key, 
    dialogue, 
    voice_mapping
)

# Concatenate with 1s gaps
final_audio = concatenate_audio_segments(
    audio_result["segments"],
    "/tmp/final_audio.mp3"
)

# Generate captions via Whisper STT
caption_handler = CaptionHandler()
subtitles_path, caption_clips = await caption_handler.process(
    final_audio,  # â† Whisper analyzes this
    captions_color="white",
    shadow_color="black"
)

# Compose final video
final_video = CompositeVideoClip([video_with_audio] + caption_clips)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‚ KEY FILES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app.py
  â†’ generate_video_script_mode() endpoint
  â†’ Orchestrates the entire flow

elevenlabs_utils.py
  â†’ generate_dialogue_audio() - Creates audio per speaker
  â†’ concatenate_audio_segments() - Combines with gaps

mediachain/.../caption_handler.py
  â†’ Calls Whisper STT
  â†’ Converts timestamps â†’ caption clips

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š DOCUMENTATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

READ THESE:

1. CAPTION_SYNC_GUIDE.md
   â†’ Full explanation with diagrams
   â†’ Whisper vs ElevenLabs comparison
   â†’ Technical details

2. elevenlabs_timestamps_util.py
   â†’ Reference implementation for ElevenLabs timestamps
   â†’ NOT currently used (optional for future)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE: How was sync happening in Topic Mode?
  1. Generate script with GPT
  2. Generate audio with OpenAI TTS
  3. Use Whisper to get timestamps
  4. Render captions

NOW: How is sync happening in Script Mode?
  1. User provides script
  2. Generate audio with ElevenLabs (per speaker)
  3. Concatenate audio (with 1s gaps)
  4. Use Whisper to get timestamps â† SAME AS BEFORE!
  5. Render captions

KEY INSIGHT:
  It doesn't matter that we use ElevenLabs for audio!
  Whisper still listens to the FINAL audio file and
  provides perfect timestamps automatically. ğŸ¯

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… STATUS: FULLY IMPLEMENTED AND WORKING!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your video generation now:
  âœ“ Uses ElevenLabs for high-quality voices
  âœ“ Adds 1s gaps between speaker changes
  âœ“ Uses Whisper for accurate caption timing
  âœ“ Combines everything into professional videos

No changes needed - it's production ready! ğŸš€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


